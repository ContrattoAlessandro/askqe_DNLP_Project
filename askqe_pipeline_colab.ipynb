{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AskQE pipeline (Qwen2.5-14B) — Colab walkthrough\n",
    "\n",
    "Questa notebook riproduce la pipeline AskQE seguendo le istruzioni e i prompt originali del repository:\n",
    "- **Generazione (QG/QA/backtranslation)** con `Qwen/Qwen2.5-14B-Instruct-AWQ` via vLLM.\n",
    "- **Valutazione** con le metriche ufficiali (F1/EM/chrF/BLEU) e SBERT (`all-MiniLM-L6-v2`).\n",
    "\n",
    "Ogni cella è pensata per essere eseguita in ordine su Google Colab con GPU L4 (22.5 GB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisiti\n",
    "- Se non hai già il repository locale, clonalo in Colab (`git clone https://github.com/dayeonki/askqe`).\n",
    "- Assicurati che gli artifact QA originali siano presenti in `QA/` se vuoi usare la sola modalità di scoring.\n",
    "- Per la generazione, il file di input JSONL deve contenere almeno il campo `en` (o `src`) e opzionalmente `tgt` / `backtranslation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Installazioni minime; aggiungi altre dipendenze del repo se necessario\n",
    "pip install -q vllm transformers sentence-transformers sacrebleu\n",
    "\n",
    "# Facoltativo: forza la versione di torch compatibile con la GPU di Colab\n",
    "# pip install -q torch --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "env-config"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path.cwd()  # cambia se il notebook non è nella root del repo\n",
    "MODEL_ID = \"Qwen/Qwen2.5-14B-Instruct-AWQ\"\n",
    "GPU_MEM_UTIL = 0.85  # tuning per L4 22.5GB\n",
    "MAX_TOKENS = 1024\n",
    "\n",
    "INPUT_JSONL = REPO_ROOT / \"data\" / \"processed\" / \"example.jsonl\"  # sostituisci con il tuo file\n",
    "GEN_OUTPUT = REPO_ROOT / \"askqe_generation_output.jsonl\"\n",
    "SCORE_CSV = REPO_ROOT / \"askqe_pipeline_baseline_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-generation"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Esegui la generazione (QG/QA/backtranslation) con Qwen2.5-14B AWQ via vLLM\n",
    "python askqe_pipeline_baseline_Judge.py \\\n",
    "  --run-generation \\\n",
    "  --input-file \"$INPUT_JSONL\" \\\n",
    "  --output-file \"$GEN_OUTPUT\" \\\n",
    "  --model-id \"$MODEL_ID\" \\\n",
    "  --gpu-mem-util $GPU_MEM_UTIL \\\n",
    "  --max-tokens $MAX_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inspect-generation"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "preview = []\n",
    "with open(GEN_OUTPUT, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 2:\n",
    "            break\n",
    "        preview.append(json.loads(line))\n",
    "\n",
    "preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-scoring"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Esegui la sola fase di scoring usando gli artifact QA già presenti in QA/\n",
    "# Cambia --model se vuoi valutare un'altra cartella (es. llama-70b, yi-9b, ecc.)\n",
    "python askqe_pipeline_baseline_Judge.py \\\n",
    "  --model llama-70b \\\n",
    "  --qa-dir \"$REPO_ROOT/QA\" \\\n",
    "  --output \"$SCORE_CSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inspect-scoring"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(SCORE_CSV)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
