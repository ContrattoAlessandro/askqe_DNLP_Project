{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AskQE end-to-end on Colab (Qwen2.5-14B AWQ via vLLM)\n",
    "\n",
    "This notebook lets you **run the full AskQE pipeline in one place**:\n",
    "\n",
    "1) Clone the original repository.\n",
    "2) Install the minimal dependencies (vLLM, transformers, sentence-transformers, sacrebleu).\n",
    "3) Materialize the `askqe_pipeline_baseline_Judge.py` utility (generation + scoring) inside Colab.\n",
    "4) Run **generation** (QG/QA/backtranslation) with `Qwen/Qwen2.5-14B-Instruct-AWQ` on a sample input.\n",
    "5) Run **scoring** with the official metrics (F1/EM/chrF/BLEU + SBERT) on existing QA artifacts.\n",
    "\n",
    "Designed for a Colab instance with an **L4 22.5 GB GPU**. You can adjust batch size and GPU utilization via script flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "pip install -q vllm transformers sentence-transformers sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone the repository\n",
    "If you already have the repo, you can skip this cell. The notebook assumes the working directory is `/content/askqe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "if [ ! -d /content/askqe ]; then\n",
    "  git clone https://github.com/dayeonki/askqe /content/askqe\n",
    "fi\n",
    "cd /content/askqe\n",
    "echo \"Repo ready at: $(pwd)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create/overwrite the pipeline script inside the repo\n",
    "This cell writes the latest `askqe_pipeline_baseline_Judge.py` (generation + scoring) directly into the cloned repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "write-script"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd /content/askqe\n",
    "cat > askqe_pipeline_baseline_Judge.py <<'PY'\n",
    "#!/usr/bin/env python3\n",
    \"\"\"\n",
    "AskQE pipeline utility.\n",
    "\n",
    "Two entrypoints are provided:\n",
    "\n",
    "1) Generation mode (`--run-generation`): run QG/QA/backtranslation with\n",
    "   Qwen2.5-14B-Instruct-AWQ via vLLM on batches of JSONL inputs.\n",
    "   - Uses the *original* AskQE prompts from the repository for atomic facts,\n",
    "     question generation, and question answering.\n",
    "   - Optional backtranslation prompt (LLM-based) if no `backtranslation`\n",
    "     field is present in the input.\n",
    "   - Outputs a JSONL file with the augmented fields.\n",
    "\n",
    "2) Scoring mode (default): reproduce the official AskQE evaluation flow by\n",
    "   consuming pre-generated QA artifacts under `QA/` and computing the\n",
    "   string-comparison metrics plus SBERT similarity, saving a CSV aggregate.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Ensure we reuse the original metric implementations and prompts.\n",
    "REPO_ROOT = Path(__file__).resolve().parent\n",
    "UTILS_PATH = REPO_ROOT / \"evaluation\" / \"string-comparison\"\n",
    "sys.path.insert(0, str(UTILS_PATH))\n",
    "\n",
    "from utils import bleu_score, chrf_score, exact_match_score, f1_score  # noqa: E402\n",
    "from QG.code.prompt import nli as qg_nli_prompt  # noqa: E402\n",
    "from QA.code.prompt import qa_prompt  # noqa: E402\n",
    "from biomqm.askqe.prompt import atomic_fact_prompt  # noqa: E402\n",
    "\n",
    "\n",
    "LANGUAGES = [\"es\", \"fr\", \"hi\", \"tl\", \"zh\"]\n",
    "PIPELINES = [\"vanilla\", \"semantic\", \"atomic\"]\n",
    "PERTURBATIONS = [\n",
    "    \"alteration\",\n",
    "    \"expansion_impact\",\n",
    "    \"expansion_noimpact\",\n",
    "    \"intensifier\",\n",
    "    \"omission\",\n",
    "    \"spelling\",\n",
    "    \"synonym\",\n",
    "    \"word_order\",\n",
    "]\n",
    "\n",
    "\n",
    "def build_vllm_components(model_id: str, gpu_mem_util: float, max_tokens: int):\n",
    "    \"\"\"Create a shared vLLM model + tokenizer and sampling params.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    llm = LLM(\n",
    "        model=model_id,\n",
    "        dtype=\"auto\",\n",
    "        max_model_len=max_tokens,\n",
    "        gpu_memory_utilization=gpu_mem_util,\n",
    "        seed=0,\n",
    "        tensor_parallel_size=1,\n",
    "    )\n",
    "    sampling = SamplingParams(\n",
    "        temperature=0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=max_tokens,\n",
    "        stop=[\"```\", \"\\n\\n\"],\n",
    "    )\n",
    "    return llm, tokenizer, sampling\n",
    "\n",
    "\n",
    "def apply_chat(tokenizer: AutoTokenizer, prompt: str) -> str:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_batch(\n",
    "    llm: LLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompts: List[str],\n",
    "    sampling: SamplingParams,\n",
    ") -> List[str]:\n",
    "    if not prompts:\n",
    "        return []\n",
    "    formatted = [apply_chat(tokenizer, p) for p in prompts]\n",
    "    outputs = llm.generate(formatted, sampling)\n",
    "    return [o.outputs[0].text.strip() for o in outputs]\n",
    "\n",
    "\n",
    "def parse_list_output(text: str) -> List[str]:\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        if isinstance(data, list):\n",
    "            return [x for x in data if isinstance(x, str)]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "\n",
    "def run_generation(args: argparse.Namespace) -> None:\n",
    "    \"\"\"Run QG/QA/backtranslation with Qwen2.5-14B-Instruct-AWQ on JSONL input.\"\"\"\n",
    "    input_path = Path(args.input_file)\n",
    "    output_path = Path(args.output_file)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    llm, tokenizer, sampling = build_vllm_components(\n",
    "        args.model_id, args.gpu_mem_util, args.max_tokens\n",
    "    )\n",
    "\n",
    "    logging.info(\"Loading input from %s\", input_path)\n",
    "    with input_path.open(\"r\", encoding=\"utf-8\") as f_in, output_path.open(\n",
    "        \"w\", encoding=\"utf-8\"\n",
    "    ) as f_out:\n",
    "        for line in f_in:\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            source = entry.get(\"en\") or entry.get(\"src\")\n",
    "            target = entry.get(\"tgt\")\n",
    "            backtranslation = entry.get(\"backtranslation\") or entry.get(\"bt_tgt\")\n",
    "            if not source:\n",
    "                continue\n",
    "\n",
    "            # Phase 1: atomic facts\n",
    "            facts_prompt = atomic_fact_prompt.replace(\"{{sentence}}\", source)\n",
    "            facts_text = generate_batch(llm, tokenizer, [facts_prompt], sampling)[0]\n",
    "            atomic_facts = parse_list_output(facts_text)\n",
    "\n",
    "            # Phase 2: question generation (NLI template)\n",
    "            qg_prompt = (\n",
    "                qg_nli_prompt.replace(\"{{sentence}}\", source).replace(\n",
    "                    \"{{atomic_facts}}\", str(atomic_facts)\n",
    "                )\n",
    "            )\n",
    "            qg_text = generate_batch(llm, tokenizer, [qg_prompt], sampling)[0]\n",
    "            questions = parse_list_output(qg_text)\n",
    "\n",
    "            # Optional backtranslation if missing\n",
    "            if not backtranslation and target:\n",
    "                bt_prompt = (\n",
    "                    \"Translate the following text to English.\\n\\n\"\n",
    "                    f\"Input: {target}\\nOutput:\"\n",
    "                )\n",
    "                backtranslation = generate_batch(\n",
    "                    llm, tokenizer, [bt_prompt], sampling\n",
    "                )[0]\n",
    "\n",
    "            # Phase 3/4: QA on source and backtranslation\n",
    "            qa_prompts = []\n",
    "            qa_prompts.append(\n",
    "                qa_prompt.replace(\"{{sentence}}\", source).replace(\n",
    "                    \"{{questions}}\", str(questions)\n",
    "                )\n",
    "            )\n",
    "            qa_prompts.append(\n",
    "                qa_prompt.replace(\n",
    "                    \"{{sentence}}\", backtranslation or \"No context provided.\"\n",
    "                ).replace(\"{{questions}}\", str(questions))\n",
    "            )\n",
    "            qa_text = generate_batch(llm, tokenizer, qa_prompts, sampling)\n",
    "            answers_src = parse_list_output(qa_text[0]) if qa_text else []\n",
    "            answers_bt = parse_list_output(qa_text[1]) if len(qa_text) > 1 else []\n",
    "\n",
    "            entry[\"atomic_facts\"] = atomic_facts\n",
    "            entry[\"questions\"] = questions\n",
    "            entry[\"backtranslation\"] = backtranslation\n",
    "            entry[\"answers_src\"] = answers_src\n",
    "            entry[\"answers_bt\"] = answers_bt\n",
    "            f_out.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    logging.info(\"Generation complete. Output -> %s\", output_path)\n",
    "\n",
    "\n",
    "def parse_answers(entry: dict) -> List[str]:\n",
    "    \"\"\"Return the normalized list of answers from a QA JSONL entry.\"\"\"\n",
    "    answers = entry.get(\"answers\", [])\n",
    "    if isinstance(answers, str):\n",
    "        try:\n",
    "            answers = json.loads(answers)\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n",
    "    if not isinstance(answers, list):\n",
    "        return []\n",
    "    return [a for a in answers if isinstance(a, str) and a.strip()]\n",
    "\n",
    "\n",
    "def load_qa_pairs(\n",
    "    predicted_path: Path, reference_path: Path\n",
    ") -> List[Tuple[List[str], List[str]]]:\n",
    "    \"\"\"Load QA pairs, aligning predicted and reference answers.\"\"\"\n",
    "    pairs: List[Tuple[List[str], List[str]]] = []\n",
    "    with predicted_path.open(\"r\", encoding=\"utf-8\") as pred_file, reference_path.open(\n",
    "        \"r\", encoding=\"utf-8\"\n",
    "    ) as ref_file:\n",
    "        for pred_line, ref_line in zip(pred_file, ref_file):\n",
    "            try:\n",
    "                pred_entry = json.loads(pred_line)\n",
    "                ref_entry = json.loads(ref_line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            pred_answers = parse_answers(pred_entry)\n",
    "            ref_answers = parse_answers(ref_entry)\n",
    "\n",
    "            if not pred_answers or not ref_answers:\n",
    "                continue\n",
    "            if len(pred_answers) != len(ref_answers):\n",
    "                continue\n",
    "\n",
    "            pairs.append((pred_answers, ref_answers))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def average_string_metrics(pairs: Iterable[Tuple[List[str], List[str]]]) -> dict:\n",
    "    \"\"\"Compute mean F1/EM/chrF/BLEU across all answer pairs.\"\"\"\n",
    "    f1_scores: List[float] = []\n",
    "    em_scores: List[float] = []\n",
    "    chrf_scores: List[float] = []\n",
    "    bleu_scores: List[float] = []\n",
    "\n",
    "    for pred_answers, ref_answers in pairs:\n",
    "        for pred, ref in zip(pred_answers, ref_answers):\n",
    "            f1_scores.append(f1_score(pred, ref, normalize=True))\n",
    "            em_scores.append(float(exact_match_score(pred, ref, normalize=True)))\n",
    "            chrf_scores.append(chrf_score(pred, ref, normalize=True))\n",
    "            bleu_scores.append(bleu_score(pred, ref, normalize=True))\n",
    "\n",
    "    if not f1_scores:\n",
    "        return {\n",
    "            \"f1\": 0.0,\n",
    "            \"em\": 0.0,\n",
    "            \"chrf\": 0.0,\n",
    "            \"bleu\": 0.0,\n",
    "            \"count\": 0,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"f1\": float(np.mean(f1_scores)),\n",
    "        \"em\": float(np.mean(em_scores)),\n",
    "        \"chrf\": float(np.mean(chrf_scores)),\n",
    "        \"bleu\": float(np.mean(bleu_scores)),\n",
    "        \"count\": len(f1_scores),\n",
    "    }\n",
    "\n",
    "\n",
    "def average_sbert_similarity(\n",
    "    pairs: Iterable[Tuple[List[str], List[str]]], model: SentenceTransformer\n",
    ") -> Tuple[float, int]:\n",
    "    \"\"\"Compute mean SBERT cosine similarity over all answer pairs.\"\"\"\n",
    "    src_texts: List[str] = []\n",
    "    tgt_texts: List[str] = []\n",
    "\n",
    "    for pred_answers, ref_answers in pairs:\n",
    "        for pred, ref in zip(pred_answers, ref_answers):\n",
    "            src_texts.append(pred)\n",
    "            tgt_texts.append(ref)\n",
    "\n",
    "    if not src_texts:\n",
    "        return 0.0, 0\n",
    "\n",
    "    src_emb = model.encode(src_texts, batch_size=64, convert_to_tensor=True)\n",
    "    tgt_emb = model.encode(tgt_texts, batch_size=64, convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(src_emb, tgt_emb).diagonal()\n",
    "    return float(similarities.mean().item()), len(similarities)\n",
    "\n",
    "\n",
    "def evaluate_combination(\n",
    "    predicted_path: Path, reference_path: Path, sbert_model: SentenceTransformer\n",
    ") -> dict | None:\n",
    "    \"\"\"Compute metrics for a single language/pipeline/perturbation combo.\"\"\"\n",
    "    if not predicted_path.exists() or not reference_path.exists():\n",
    "        logging.warning(\"Missing files: %s or %s\", predicted_path, reference_path)\n",
    "        return None\n",
    "\n",
    "    pairs = load_qa_pairs(predicted_path, reference_path)\n",
    "    if not pairs:\n",
    "        logging.warning(\"No aligned QA pairs found for %s\", predicted_path)\n",
    "        return None\n",
    "\n",
    "    string_metrics = average_string_metrics(pairs)\n",
    "    sbert_mean, sbert_count = average_sbert_similarity(pairs, sbert_model)\n",
    "\n",
    "    return {\n",
    "        **string_metrics,\n",
    "        \"sbert\": sbert_mean,\n",
    "        \"sbert_count\": sbert_count,\n",
    "    }\n",
    "\n",
    "\n",
    "def build_output_row(\n",
    "    language: str, perturbation: str, pipeline: str, metrics: dict\n",
    ") -> List[str | float | int]:\n",
    "    return [\n",
    "        language,\n",
    "        perturbation,\n",
    "        pipeline,\n",
    "        metrics[\"count\"],\n",
    "        metrics[\"f1\"],\n",
    "        metrics[\"em\"],\n",
    "        metrics[\"chrf\"],\n",
    "        metrics[\"bleu\"],\n",
    "        metrics[\"sbert\"],\n",
    "        metrics[\"sbert_count\"],\n",
    "    ]\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=(\n",
    "            \"AskQE pipeline utility. \"\n",
    "            \"Use --run-generation for QG/QA/backtranslation with Qwen2.5-14B AWQ, \"\n",
    "            \"or omit to run the scoring/aggregation step.\"\n",
    "        )\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--run-generation\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Run QG/QA/backtranslation with Qwen2.5-14B-Instruct-AWQ.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input-file\",\n",
    "        type=Path,\n",
    "        help=\"JSONL input file with `en` (or `src`) and optional `tgt` / `backtranslation`.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-file\",\n",
    "        type=Path,\n",
    "        default=REPO_ROOT / \"askqe_generation_output.jsonl\",\n",
    "        help=\"Path to write augmented JSONL when --run-generation is set.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model-id\",\n",
    "        default=\"Qwen/Qwen2.5-14B-Instruct-AWQ\",\n",
    "        help=\"Model ID to load with vLLM for generation.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gpu-mem-util\",\n",
    "        type=float,\n",
    "        default=0.85,\n",
    "        help=\"vLLM gpu_memory_utilization (tune for L4 22.5GB).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-tokens\",\n",
    "        type=int,\n",
    "        default=1024,\n",
    "        help=\"Maximum new tokens for generation (vLLM SamplingParams).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        default=\"llama-70b\",\n",
    "        help=\"Model subdirectory under QA/ containing generated answers.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--qa-dir\",\n",
    "        type=Path,\n",
    "        default=REPO_ROOT / \"QA\",\n",
    "        help=\"Root directory containing QA outputs (default: ./QA).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        type=Path,\n",
    "        default=REPO_ROOT / \"askqe_pipeline_baseline_results.csv\",\n",
    "        help=\"CSV file to write aggregated metrics.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(levelname)s - %(message)s\", level=logging.INFO, force=True\n",
    "    )\n",
    "\n",
    "    if args.run_generation:\n",
    "        if not args.input_file:\n",
    "            parser.error(\"--input-file is required when using --run-generation\")\n",
    "        run_generation(args)\n",
    "        return\n",
    "\n",
    "    sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    qa_root = args.qa_dir / args.model\n",
    "    output_rows: List[List[str | float | int]] = []\n",
    "\n",
    "    for language in LANGUAGES:\n",
    "        for pipeline in PIPELINES:\n",
    "            for perturbation in PERTURBATIONS:\n",
    "                predicted_path = qa_root / f\"{language}-{pipeline}-{perturbation}.jsonl\"\n",
    "                reference_path = qa_root / f\"en-{pipeline}.jsonl\"\n",
    "\n",
    "                metrics = evaluate_combination(\n",
    "                    predicted_path, reference_path, sbert_model\n",
    "                )\n",
    "                if metrics is None:\n",
    "                    continue\n",
    "\n",
    "                row = build_output_row(language, perturbation, pipeline, metrics)\n",
    "                output_rows.append(row)\n",
    "                logging.info(\n",
    "                    \"%s | %s | %s -> F1: %.3f, EM: %.3f, chrF: %.3f, BLEU: %.3f, SBERT: %.3f (%d pairs)\",\n",
    "                    language,\n",
    "                    pipeline,\n",
    "                    perturbation,\n",
    "                    metrics[\"f1\"],\n",
    "                    metrics[\"em\"],\n",
    "                    metrics[\"chrf\"],\n",
    "                    metrics[\"bleu\"],\n",
    "                    metrics[\"sbert\"],\n",
    "                    metrics[\"count\"],\n",
    "                )\n",
    "\n",
    "    header = [\n",
    "        \"language\",\n",
    "        \"perturbation\",\n",
    "        \"pipeline\",\n",
    "        \"num_pairs\",\n",
    "        \"f1\",\n",
    "        \"em\",\n",
    "        \"chrf\",\n",
    "        \"bleu\",\n",
    "        \"sbert\",\n",
    "        \"sbert_pairs\",\n",
    "    ]\n",
    "    args.output.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with args.output.open(\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(output_rows)\n",
    "\n",
    "    logging.info(\"Saved results to %s\", args.output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "PY\n",
    "chmod +x askqe_pipeline_baseline_Judge.py\n",
    "python -m compileall askqe_pipeline_baseline_Judge.py >/dev/null\n",
    "echo \"Script written and byte-compiled.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick sanity check\n",
    "List the repo and confirm the script is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ls-repo"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/askqe\n",
    "ls -l askqe_pipeline_baseline_Judge.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Prepare a small sample for generation\n",
    "To avoid long runs, you can take a tiny slice of the dataset. Adjust `SAMPLE_LINES` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sample-input"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/askqe\n",
    "SAMPLE_LINES=2\n",
    "INPUT_FULL=data/processed/en-es.jsonl\n",
    "INPUT_SAMPLE=/content/askqe/sample_en_es.jsonl\n",
    "head -n $SAMPLE_LINES \"$INPUT_FULL\" > \"$INPUT_SAMPLE\"\n",
    "echo \"Sample saved to $INPUT_SAMPLE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run generation (QG/QA/backtranslation) with Qwen2.5-14B AWQ\n",
    "- Uses vLLM with `gpu_memory_utilization=0.85` tuned for L4 22.5 GB.\n",
    "- Input must contain `en` (or `src`) and optional `tgt` / `backtranslation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-generation"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/askqe\n",
    "python askqe_pipeline_baseline_Judge.py \\\n",
    "  --run-generation \\\n",
    "  --input-file /content/askqe/sample_en_es.jsonl \\\n",
    "  --output-file /content/askqe/askqe_generation_output.jsonl \\\n",
    "  --model-id Qwen/Qwen2.5-14B-Instruct-AWQ \\\n",
    "  --gpu-mem-util 0.85 \\\n",
    "  --max-tokens 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect generation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inspect-gen"
   },
   "outputs": [],
   "source": [
    "import json, itertools\n",
    "\n",
    "path = \"/content/askqe/askqe_generation_output.jsonl\"\n",
    "preview = []\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in itertools.islice(f, 2):\n",
    "        preview.append(json.loads(line))\n",
    "preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run scoring on existing QA artifacts (official metrics)\n",
    "This uses the pre-generated QA files under `QA/` (default: `QA/llama-70b`). Adjust `--model` and `--qa-dir` if you want to point to other folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-scoring"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/askqe\n",
    "python askqe_pipeline_baseline_Judge.py \\\n",
    "  --model llama-70b \\\n",
    "  --qa-dir /content/askqe/QA \\\n",
    "  --output /content/askqe/askqe_pipeline_baseline_results.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect scoring results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inspect-score"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/askqe/askqe_pipeline_baseline_results.csv\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
